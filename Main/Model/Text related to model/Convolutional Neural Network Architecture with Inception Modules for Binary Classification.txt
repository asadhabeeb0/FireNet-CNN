The size of input image is 224×224×3 pixels on which 64 kernels of size 7×7 are applied with stride 2, resulting in 64 feature maps of size 112×112 . Then, a max pooling with kernel size 3×3 and stride 2 is used to filter out maximum activations from previous 64 feature maps. Next, another convolution with filter size 3×3 and stride 1 is applied, resulting in 192 feature maps of size 56×56 . This is followed by another max pooling layer with kernel size 3×3 and stride 2, filtering discriminative rich features from less important ones. Next, the pipeline contains two inception layers (3a) and (3b). The motivational reason of such inception modulus assisted architecture is to avoid uncontrollable increase in the computational complexity and networks’ flexibility to significantly increase the number of units at each stage. To achieve this, dimensionality reduction mechanism is applied before computation-hungry convolutions of patches with larger size. The approach used here is to add 1×1 convolutions for reducing the dimensions, which in turn minimizes the computations. Such mechanism is used in each inception module for dimensionality reduction. Next, the architecture contains a max pooling layer of kernel size 3×3 with stride 2, followed by four inception modules 4 (a-e). Next, another max pooling layer of same specification is added, followed by two more inception layers (5a and 5b). Then, an average pooling layer with stride 1 and filter size 7×7 is introduced in the pipeline, followed by a dropout layer to avoid overfitting. At this stage, we modified the architecture according to our classification problem by keeping the number of output classes to 2 i.e., fire and non-fire.